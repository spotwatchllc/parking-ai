general:
  project_name: speech_enhancement_project
  logs_dir: logs
  saved_models_dir: saved_models 
  gpu_memory_limit: 0.5 # Fraction of GPU's memory to use.
  display_figures: True # Set to True to display figures. Figures are saved even if set to False.

operation_mode: training
model:
  model_type: STFTTCNN # For training
  state_dict_path: # For training and evaluating torch models
  onnx_path: # For quantization, evaluation, benchmarking and deployment only

model_specific:
  # Parameters specific to your model type, e.g. n_blocks, tcn_latent_dim for STFT-TCNN
  n_blocks: 2
  num_layers: 3
  in_channels: 257
  tcn_latent_dim: 512
  init_dilation: 2
  mask_activation: "sigmoid"

dataset:
  name: valentini # Or "custom"
  root_folder: /local/datasets/Valentini # Root folder of dataset
  n_speakers: 56 # For Valentini, 28 or 56 speaker dataset. Does nothing if name is "custom"
  file_extension: '.wav' # Extension of audio files. Valentini dataset uses .wav

  # For the following parameters, leave empty to include all samples.
  # You can set them to a specific n째 of samples (integer), 
  # or a fraction (float) of their respective sets.

  num_training_samples:  # N째 of samples or fraction of training set to include in training set
  num_validation_samples: 100 # N째 of samples or fraction of training set to include in validation set.
  num_test_samples:  # N째 of samples or fraction of test set to include in test set
  shuffle: True # If True, training dataset is shuffled each epoch
  random_seed: 42 # Random seed used for sampling. If left empty, sampling is not seeded.

  # The following parameters are to be used for custom datasets. 
  # You can leave them empty if "name" is "valentini", and the default paths will be used.
  clean_train_files_path:
  clean_test_files_path:
  noisy_train_files_path:
  noisy_test_files_path:

preprocessing:
  pipeline_type: LibrosaSpecPipeline # Do not change if unsure.
  peak_normalize: False
  sample_rate: 16000
  n_fft: 512
  hop_length: 160
  win_length: 400
  window: hann
  center: True
  power: 1

training:
 device: cuda:0
 epochs: 100

 # Optimizer parameters
 optimizer: Adam # Can use any torch optimizer
 # Add additional arguments to be passed to the optimizer to this dict
 optimizer_arguments: {lr: 0.001}

 # Training loop parameters
 loss: spec_mse # one of ["wave_mse", "wave_snr", "wave_sisnr", "spec_mse"]
 batching_strategy: pad # one of ["trim", "pad"]

 # Dataloader parameters
 num_dataloader_workers: 4 # Should divide batch_size Set to 0 if on Windows and having issues.
 batch_size: 16 # Recommend keeping it low if using `trim` batching strategy.

 # Reference metric used for early stopping, and saving the best model produced during training
 reference_metric: pesq # One of 'train_loss', 'wave_mse', 'stoi', 'pesq', 'snr', 'si-snr'
 
 # Early stopping parameter
 early_stopping: True # True/False to enable/disable
 early_stopping_patience: 50 # Number of epochs with no improvement in reference_metric before training stops.


#  Regularization parameters
#  Comment the following block to remove all regularization during training.
 regularization:
  weight_clipping_max:  # Leave empty to disable weight clipping
  activation_regularization:  # Leave empty to disable activation regularization
  act_reg_layer_types: [Conv1d, DepthwiseSeparableConv] # Type of layers to regularize
  act_reg_threshold: 50.0 # Will not penalize activations below threshold. Leave empty to penalize all activations.
  penalty_type: l2 # "l1" or "l2"

 # Checkpointing and snapshotting parameters
 save_every: 2 # Saves a checkpoint and snapshot every n epochs
 snapshot_path:  # Set this to a previously saved training snapshot to resume training.
 ckpt_path: ckpts/ # Path to checkpoints, appended to general.saved_model_dir
 logs_filename: training_logs.csv # appended to general.logs_dir

 # ONNX exporting parameters
 opset_version: 17

mlflow:
  uri: ./experiment_outputs/mlruns

hydra:
  run:
    dir: ./experiment_outputs/${now:%Y_%m_%d_%H_%M_%S}
  




